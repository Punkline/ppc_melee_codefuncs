-==-

Memory Management Functions

mem.alloc  # Allocate dynamically managed memory
mem.allocz # - alloc + zero out allocation
args: rSize  # single arg syntax -- for generic heap allocations
args: rID, rSize  # 2 args -- for allocating to cache regions
 <-- rAlloc, rMeta, rAligned, rSize, rID
 <-- cr1.lt: bIsAvailable,  cr1.gt: bIsARAM,  cr1.eq: bIsHeap
 # returns rAlloc as alloc address, and rAligned as real size
 # - if bAvailable is False -- rAlloc will be null

# rMeta is heap/cache metadata:
rMeta  # for OSHeap Fragment Metadata (ID 0)...
0x0 = Previous Fragment
0x4 = Next Fragment
0x8 = Fragment size

rMeta  # for Preload Cache Stack Frame Metadata...
0x0 = Next frame
0x4 = Point to cache alloc
0x8 = Frame size

# rID references one of the defeind caches/object heap
rID  # Default HSD Memory Region IDs
0 = HEAP  -- RAM  -- HSD Object Heap (OSHeap[1])
1 = CACHE -- ARAM -- Excess ARAM fragment
2 = CACHE -- RAM  -- Priority Archive Cache
3 = CACHE -- RAM  -- Main Archive Cache
4 = CACHE -- RAM  -- Preload Archive Cache
5 = CACHE -- ARAM -- Aux Preload Archive Cache


mem.free  # Free an allocation
args: rAlloc  # dynamic heap allocation (freed to heap[1])


mem.push  # change the size defs of arena/preload regions
args: rID, rAdd  # IDs 0/1 are replaced with ArenaLo/Hi
 <-- rSize, rAddr  # rAddr is only returned on Arena push
 # NOTE: only takes place on scene changes
 # NOTE: Arena can only be changed on game start


mem.info  # has 3 input variations...
args: rAddr  # return info about region and allocation (SRAM)
 <-- rID, rMem, rHeap, rCache, rAlloc, rSize, rMeta, rOffset, rStatic, rString
 <-- cr1.lt: bInRegion, cr1.gt: bIsAllocated, cr1.eq: bIsHeap
 # rOffset measures input rAddr relative to rAlloc

args: rID, rSize  # or return info about first found allocation
 <-- rID, rMem, rHeap, rCache, rAlloc, rSize, rMeta, rOffset, rStatic, rString
 <-- cr1.lt: bIsAvailable, cr1.gt: bIsARAM, cr1.eq: bIsHeap

args: rID, -1  # or return info about current remaining space
 <-- rID, rMem, rHeap, rCache, rFCount, rFBig, rFTotal, rACount, rABig, rATotal
 <-- cr1.lt: bIsAvailable, cr1.gt: bIsARAM, cr1.eq: bIsHeap
 # NOTE: this 3rd format is returned if no target is found

rStatic  # Static region IDs 0 ... 10:
"HEAD", "TEXT", "DATA", "BSS", "SDATA", "TOC", "TEMP", "LOW", "HSD", "HIGH", "FST"
# NOTE: HSDMem encompasses all preload caches, and HSDHeap


mem.ID  # return information about memory region, by ID
args: rID  # rID is a number between 0 and 5
 <-- rID, rMem, rHeap, rCache, rStart, rSize, rDef, rDefSize
 <-- cr1.lt: bIsAvailable, cr1.gt: bIsARAM, cr1.eq: bIsHeap

 rMem  # for IDs 0 ... 5
 0x0 = Heap ID  (if applicable)
 0x4 = point to Cache Boundary Descriptor (if applicable)
 0x8 = point to Address of Boundary Start
 0xC = Size of Region
 0x10 = Region Lo behavior
 0x14 = intialized flag? (1 = after init?)
 0x18 = disabled flag?   (1 = not available)

 rHeap  # for ID 0, or any region modified to use OSHeap
 0x0 = total bytes (in initial fragment)
 0x4 = point to first Free Fragment Metadata
 0x8 = point to first Allocated Fragment Metadata

 rCache  # for IDs 1 ... 5 -- mostly used for preloading
 0x0 = to next descriptor??
 0x4 = point to region boundary low pointer
 0x8 = point to region boundary high pointer
 0xC = point to first Cache Frame Metadata

 rDef  # for IDs 2 ... 5 -- defined in the DOL
 0x0 = Memory Region ID (0 ... 5)
 0x4 = Behavior ID
 0x8 = Region ID to come after
 0xC = Region Size
[Punkline]

<mem.alloc> NTSC 1.02
# Allocate a piece of memory available from one of 6 partitions

# --- args:  Common
# r3 = number of bytes (> 5)
#    - all allocations are rounded up to the nearest 0x20 byte ceiling alignment
#    - all allocations are collapsed at end of scene, and use OSHeap[1] -- the HSD Object heap

# --- args:  Specific Partition
# r3 = HSD Memory Region ID (<= 5)
#    - 0  -- HEAP  -- RAM  -- HSD Object Heap (OSHeap[1])  - minor scene persistence
#    - 1  -- CACHE -- ARAM -- Excess ARAM fragment         - minor scene persistence
#    - 2  -- CACHE -- RAM  -- Priority Archive Cache       - full game persistence
#    - 3  -- CACHE -- RAM  -- Main Archive Cache           - full game persistence
#    - 4  -- CACHE -- RAM  -- Preload Archive Cache        - major scene persistence
#    - 5  -- CACHE -- ARAM -- Aux Preload Archive Cache    - major scene persistence
# r4 = number of bytes
#    - all allocations are rounded up to the nearest 0x20 byte ceiling alignment
#    - allocation volatility is determined by the region

# --- returns:
# cr1.lt = bIsAvailable -- there is metadata available for this region
# cr1.gt = bIsARAM      -- ARAM can't be read/written to directly by the CPU
# cr1.eq = bIsHeap      -- uses OSHeap metadata structure instead of HSD Cache metadata structure
# r3 = address of allocation
# r4 = Metadata for this allocation (different based on Heap or Cache)
# r5 = number of bytes allocated (after alignment)
# r6 = number of bytes allocated and zeroed (before alignemnt)
# r7 = HSD Memory Region ID

 # for OSHeap Fragment Metadata (ID 0)...
 #   - 0x0 = Previous Fragment
 #   - 0x4 = Next Fragment
 #   - 0x8 = Fragment size

 # for Preload Cache Stack Frame Metadata...
 #   - 0x0 = Next frame
 #   - 0x4 = Point to cache alloc
 #   - 0x8 = Frame size
li r5, 0
b <mem.__alloc_handler>
<mem.allocz> NTSC 1.02
li r5, 1
b <mem.__alloc_handler>
<mem.__alloc_handler> NTSC 1.02
.include "punkpc.s"
punkpc ppc
prolog +0x20, xReturns, rID, rSize, rAlign, rMeta, rAlloc, rReturns, rCR
enum (b31), -1, bZero, bTry
mfcr rCR
cmpwi r3, 5
mtcrf 0x01, r5
li rID, 0
mr rSize, r3
li rAlloc, 0
bgt+ 0f
  mr rID, r3
  mr rSize, r4
  # This initialization sets up both the 1 arg and 2 arg input syntaxes
  # - now both cases can be treated with the same code

0:
addi r0, rSize, 0x1F
rlwinm. rAlign, r0, 0, ~0x1F
# If not already aligned, pad the size to a ceiling 0x20 byte alignment

mr r3, rID
mr r4, rSize
bl <mem.info>
mcrf 6, 1
enum (cr6.lt), +1, bIsAvailable, bIsARAM, bIsHeap
# bool info about memory query is saved to cr6

bf- bIsAvailable, _return
crandc bZero, bZero, bIsARAM
# if cache or heap is currently not available, then return nulls

mr r3, rID
mr r4, rSize
bl 0x80015bd0
# create allocation

mr rAlloc, r3
subi rMeta, rAlloc, 0x20
bt+ bIsHeap, 0f
  mr rMeta, r3
  lwz rAlloc, 4(r3)
0: # get rMeta depending on allocation type

bf+ bZero, _return
  mr r3, rAlloc
  mr r4, rSize
  bl 0x8000c160
  # zero out region, if flagged

_return:
addi rReturns, sp, sp.xReturns
mcrf 1, 6
mtcrf 0xbf, rCR
stmw rAlloc, 0(rReturns)
lswi r3, rReturns, 0x14
epilog
blr


<mem.free> NTSC 1.02
# Free a dynamically allocated fragment from the Object Heap
# - can only be applied to allocations that have contiguous OSHeap metadata at offset -0x20

# --- args:
# r3 = start of allocation from OSHeap[1]
b 0x8037f1b0


<mem.push> NTSC 1.02
# Push the size of a given memory region by adding/subtracting bytes to the size definition
# - This 'pushes' or 'pops' memory to a region on next scene change
# - OSArena pushes can only be made before the first scene has begun

# --- args:
# r3 = rID  NOTE: 0 and 1 can't be pushed, because they are remainders of total SRAM and DRAM
#           - 0 has been replaced with 'OSArenaLo'
#           - 1 has been replaced with 'OSArenaHi'
# r4 = bytes to add/subtract to Region

# --- return:
# r3 = address of MemDef (or OSArena Pointer variable)
# r4 = new byte total (or new OSArena push start address)
.include "melee"
melee mem
punkpc ppc
regs (r3), rDef, rSize, (r3), rID, rPush, (r4), rPoint
subic. r0, rID, 2
blt- _OS_Arena_Definition

  _HSD_Memory_Definition:
  load rDef, MemDef.addr
  mulli r0, r0, MemDef.size
  add rDef, rDef, r0
  lwz r0, MemDef.xSize(rDef)
  add rSize, rPush, r0
  stw rSize, MemDef.xSize(rDef)
  blr

  _OS_Arena_Definition:
  lwz r5, r13.xOSArenaHi(r13)
  lwz r0, r13.xOSArenaLo(r13)
  sub r5, r5, r0
  sub. r0, r5, rPush
  bgt+ 0f; li rPush, 0; 0:
  # if push would be too large for OSArena, then abort

  cmpwi rID, 0
  addi rDef, r13, r13.xOSArenaLo
  beq- 0f;  addi rDef, r13, r13.xOSArenaHi; 0:
  # else, if ID is 1, then negate push size for adding to descending pointer

  lwz rPoint, 0(rDef)
  add r5, rPoint, r0
  stw r5, 0(rDef)
  beqlr-
    mr rPoint, r5
    blr # if ArenaHi, then the top of new push is at start of subtracted offset

<mem.ID> NTSC 1.02
.include "melee"
melee mem
punkpc ppc

# register/bool names:
regs (r3), rID, rMem, rHeap, rCache, rStart, rSize, rDef, rDefSize
enum (cr1.lt), +1, bIsAvailable, bIsARAM, bIsHeap,  (lt), bInRange
rGlob = rDef

_start_func:
cmpwi cr1, rID, 0
load rGlob, MemGlob.addr
lwz r5, MemGlob.xIDMax(rGlob)
addi rDef, rGlob, MemGlob.size
cmplw rID, r5
crnor bInRange, gt, cr1.lt
# bInRange encodes a check to see if given r3 ID is within this counted range

li rHeap, 0
li rCache, 0
li rStart, 0
li rSize, 0
li rDefSize, 0
mtcrf 0x40, rHeap
bt+ bInRange, _region
# initialize all return values as null, for case of oob

  _arena:
  li rMem, 0
  # if the given ID is invalid (like -1), then just return info about OSArena instead
  # - bool bIsAvailable is still returned false, to reflect bad input ID
  #   - additionally, rMem will be null

  lwz rSize, r13.xOSArenaHi(r13)
  lwz rStart, r13.xOSArenaLo(r13)
  sub rSize, rSize, rStart
  blr # High is represented as a delta size
  # - this means that if rSize is returned as '0' -- then the arena is closed

_region:
load rMem, MemDesc.addr
mulli r0, rID, MemDesc.size
add rMem, rMem, r0
lwz r0, MemDesc.xDisabled(rMem)
cmpwi cr1, r0, 1
crand bIsAvailable, lt, bInRange
# if xDisabled var is 0 and bInRange, then bIsAvailable

# lwz r0, MemDesc.xInit(rMem)
# cmpwi r0, 1
# crand bIsAvailable, bIsAvailable, eq
# bIsAvailable &= bIsInitialized

lwz r0, MemDesc.xBehavior(rMem)
cmpwi r0, 2
crmove bIsARAM, gt
# ARAM IDs are 3 and 4 -- so if >2, expect ARAM (DRAM) cache instead of normal SRAM

lwz r0, MemDesc.xHeapID(rMem)
cmpwi r0, -1
crmove bIsHeap, gt
# If a Heap ID is >=0, then it is referencing an OSHeap to divert to instead of an HSD cache

bflr- bIsAvailable
# null init register values and calculated bools are returned if region is not available
# - if in range, initialized, and not disabled -- then registers are filled with information

_return_info:
bf- bIsHeap, 0f
  lwz rHeap, r13.xOSHeapDescs(r13)
  mulli r0, r0, HeapDesc.size  # r0 still holds xHeapID value
  add rHeap, rHeap, r0
  b 1f
0: lwz rCache, MemDesc.xCache(rMem)
1: # if bIsHeap then return heap info; else return cache info
# - the opposite case will have a corresponding null in its return register

subic. r0, rID, 2
blt+ 0f
  mulli r0, r0, MemDef.size
  add rDef, rDef, r0
  lwz rDefSize, MemDef.xSize(rDef)
  b 1f
0: li rDef, 0
1: # if ID is >=2, then it has a static definition
# - else, return a null for rDef -- to avoid returning arbitrary data

lwz rStart, MemDesc.xStart(rMem)
lwz rSize, MemDesc.xSize(rMem)
blr # return region boundary summary from memory descriptor




<mem.info> NTSC 1.02
.include "melee"
melee mem
punkpc ppc
prolog rData, rArg, rArg2, rHigh, rLow, rThis, rNext, rString, rStatic, rMeta, rOffset, rSize, rStart, rReturns, rCR, xBiggest, +0x40, xReturns
  # saved registers, and temp workspace in sp

  regs (r3), rID, rMem, rHeap, rCache, rFCount, rFBig, rFTotal, rACount, rABig, rATotal
  # returned registers (for failed parse)

  enum (cr1.lt), +1, bIsAvailable, bIsARAM, bIsHeap, (eq), bInvalid
  # bools used in parse/return

  enum (cr2.lt), +1, bIsValid, bMatchRAM, bMatchType, bMatch, bPass, bCheckSize
  # bools used in parse

  enum.enum_conc "datahead.",, (0), +1, xBoundaries, xStrings, xCount, size
  # - custom data in rData has a header with 3 bytes, followed by an array of byte pointers


  rGlob = rMem
  bInRegion = bIsAvailable
  bIsAllocated = bIsARAM
  # aliases

  mfcr rCR
  mr rArg, r3
  mr rArg2, r4
  lis rHigh, 0x8180
  lis rLow, 0x0100
  addi rReturns, sp, sp.xReturns
  cmpw cr2, rArg, rHigh
  cmplw rArg, rLow
  cror bIsValid, cr2.lt, lt
  # validity requires that r3 be within ID, SRAM or DRAM integer range

  bl <mem.ID>  # returns rID, rMem, rHeap, rCache, and cr1 bools
  cmpwi rMem, 0
  cror bIsAvailable, lt, bIsValid
  mcrf 2, 1
  # combine check for valid ID with previous check for Addr ranges...
  # - we move the cr2 bool over to cr1 before copying it back over after combination with whole CRF

  crnot bCheckSize, eq
  crset bPass
  crclr bMatch
  # initialize other bools

  _init_nulls:
  load rFCount, 0, 0, 0, 0, 0, 0
  stswi rFCount, rReturns, 6<<2
  lswi rStart, rReturns, 6<<2
  # initialize the remaining nulls for return, or start of loop
  # - if bIsValid == False, then r4, r5, r6 are null from <mem.ID> return

  bf- bIsValid, _return
  # At this point, bIsValid represents a Validated ID, SRAM Addr, or DRAM Addr...
  # - if none of those are detected, then just return the above nulls as an error

  bt+ bCheckSize, _loop
  # ... else, branch into corresponding initialization using recognized input type

    li rID, -1
    # if checking for addresses, then we don't know what region it is in
    # - we check each region of matching RAM type detected in bMatchRAM

    cmpwi rArg, 0
    crnot bMatchRAM, lt
    # Set MatchRAM bool according to sign in rArg
    # - True means it is ARAM (DRAM)



_loop:
  crnot bPass, bPass
  # - 2 passes, indexed by bool 'bPass'

  # setup logic:
  bt+ bCheckSize, _check_size_start
    bt- bPass, 1f

      _next_ID:
      addi rID, rID, 1
      bl <mem.ID>
      cmpwi rMem, 0
      crmove bIsValid, lt
      bf- bIsValid, _init_nulls
      # if all valid IDs are parsed without a match, then return nulls

      creqv bIsValid, bIsARAM, bMatchRAM
      crand bIsValid, bIsAvailable, bIsValid
      bf- bIsValid, _next_ID
      # if region doesn't match RAM type of query, then skip it

    1:
    crclr bIsAllocated
    crclr bInRegion
    bt+ bIsHeap, _heap_addr



      _cache_addr:
      lwz rLow, CacheDesc.xLow(rCache)
      cmpw rArg, rLow
      lwz rHigh, CacheDesc.xHigh(rCache)
      crnot bMatch, lt
      cmpw rArg, rHigh
      crand bMatch, bMatch, lt
      lwz rMeta, CacheDesc.xMeta(rCache)
      crset bInRegion
      li rStart, 0
      bf+ bMatch, _next_ID

        # for cache addr subloop, we just check each alloc
        # - if alloc isn't found, but still in range -- then arg is part of free space
        2:  cmpwi rMeta, 0
        bge- _cache_addr_is_free
        lwz rStart, CacheMeta.xAlloc(rMeta)
        cmpw rArg, rStart
        blt+ 3f
          lwz rSize, CacheMeta.xSize(rMeta)
          add rHigh, rSize, rStart
          cmpw rArg, rHigh
          blt- _cache_addr_is_alloc
        3: lwz rMeta, CacheMeta.xNext(rMeta)
        b 2b

        _cache_addr_is_alloc:
        crset bIsAllocated

        _cache_addr_is_free:
        lwz rHigh, CacheDesc.xHigh(rCache)
        sub rSize, rHigh, rStart
        _set_offset:
        sub rOffset, rArg, rStart
        b _return



    _heap_addr:
    li rStart, 0
    lwz rMeta, HeapDesc.xAlloc(rHeap)
    bf+ bPass, 1f
      lwz rMeta, HeapDesc.xFree(rHeap)
    1: # for heap addr subloop, we need to check both allocated and free fragments one by one
       # - this is done in 2 passes, signified by the bPass bool

      cmpwi rMeta, 0
      bge- _loop
      cmpw rArg, rMeta
      lwz rSize, HeapMeta.xSize(rMeta)
      crnot bMatch, lt
      add rHigh, rMeta, rSize
      cmpw rArg, rHigh
      addi rStart, rMeta, 0x20
      crand bMatch, bMatch, lt
      # we include 0x20 byte meta header in alloc space for complete contiguity

      bt- bMatch, _heap_addr_match
        lwz rMeta, HeapMeta.xNext(rMeta)
        b 1b

    _heap_addr_match:
    crset bInRegion
    crnot bIsAllocated, bPass
    b _set_offset
    # Heap address match measures each individual fragment
    # - this includes hacked fragments in regions normally OOB to the heap



  _check_size_start:
  # if checking for size, then rArg2 contains a size query, and rID is the selected region

  li r0, 0
  stw r0, sp.xBiggest(sp)
  # - biggest sample is reset to null

  bt+ bIsHeap, _heap_size

    _cache_size:
    lwz rMeta, CacheDesc.xMeta(rCache)
    cmpwi rMeta, 0
    lwz rSize, MemDesc.xSize(rMem)
    li rATotal, 0
    bge- 1f
    lwz rNext, CacheMeta.xNext(rMeta)
    0: cmpwi rNext, 0
      bge- 1f
        addi rACount, rACount, 1
        lwz rThis, CacheMeta.xSize(rMeta)
        add rATotal, rATotal, rThis
        cmpw rThis, rABig
        ble+ 2f;
          mr rABig, rThis
          stw rMeta, sp.xBiggest(sp)
          # sample biggest by storing metadata pointer in temp workspace

        2:
        mr rMeta, rNext
        lwz rNext, CacheMeta.xNext(rMeta)
        b 0b  # count bytes in each push of the cache stack

    1: sub rFTotal, rSize, rATotal
    li rFBig, 0
    li rFCount, 0
    # calculate total free bytes by subtracting counted bytes from defined total

    cmplw rArg2, rFTotal
    lwz rABig, sp.xBiggest(sp)
    crmove bIsAvailable, lt
    crmove bMatch, bIsAvailable
    # finally, see if query size fits within calculated free bytes

    bf- bIsAvailable, _return
    # if not, then return the current state of the parser

    lwz rHigh, CacheDesc.xHigh(rCache)
    mr rSize, rFTotal
    sub rStart, rHigh, rSize
    # - this returns the rF* and rA* for information about the memory region

    b _return
    # else, return match values, with bIsAvailable = True
    # - since there is no real fragment for free cache space, we subtract remaining bytes from ceil

  _heap_size:
  lwz rMeta, HeapDesc.xFree(rHeap)
  crclr bMatch
  bf+ bPass, 0f
    lwz rMeta, HeapDesc.xAlloc(rHeap)
    mr rFCount, rACount
    mr rFTotal, rATotal
    mr rFBig, rABig
    li rATotal, 0
    li rACount, 0
    # search for free heap fragments first, to confirm if space is/isn't available

    0:
      cmpwi rMeta, 0
      bge- 0f
      lwz rSize, HeapMeta.xSize(rMeta)
      bt- bPass, 1f
        cmplw rArg2, rSize
        crmove bMatch, lt

      1:
      cmpw rSize, rABig
      addi rACount, rACount, 1
      blt+ 1f;
        mr rABig, rSize
        stw rMeta, sp.xBiggest(sp)
        # sample biggest by storing metadata pointer in temp workspace

      1:
      add rATotal, rATotal, rSize
      #addi rATotal, rATotal, 0x20
      addi rStart, rMeta, 0x20
      lwz rMeta, HeapMeta.xNext(rMeta)
      bt- bMatch, 0f
      b 0b  # check for match

    0: crandc bIsAvailable, bMatch, bPass
    cror bPass, bPass, bMatch
    lwz rABig, sp.xBiggest(sp)
    bf+ bPass, _loop
    # if found in first pass, flag bIsAvailable, else set to false
    # else, second pass checks allocs and does not flag bIsAvailable on match












  _return:
  crand bIsValid, bIsValid, bMatch
  mfcr r0
  rlwimi rCR, r0, 0, 0x0F000000
  bt- bCheckSize, _exit


  # Additionally, we want to return a static boundary if successfuly query is in SRAM:

  _check_for_static_boundary:
  stswi rStart, rReturns, 6<<2
  bt+ bIsValid, 0f; mr rStart, rArg; 0:
  bt- bIsARAM, 0f
  # the audio samples in ARAM are not well enough documented to label them (at this time)
  # - if info is for ARAM address (DRAM), then no static boundary ID is given

    addi rReturns, rReturns, 6<<2
    stswi r3, rReturns, 4<<2
      mr r3, rStart
      bl <mem.static>
      stw mem.static.rStart, -8(rReturns)
      stw mem.static.rString, -4(rReturns)
    lswi r3, rReturns, 4<<2
    addi rReturns, rReturns, -6<<2
    # sloppy return context planning

  0:
  lswi mem.info.rStart, rReturns, 6<<2
  # prepare all returned registers

_exit:
mtcr rCR
epilog
blr


<mem.static> NTSC 1.02
.include "melee"; punkpc; melee mem
regs (r3), rArg, rCount,  (r4), rID, rAddr, rString, rArr, rData, rOS
enum.enum_conc "datahead.",, (0), +1, xBoundaries, xStrings, xCount, size

lis r8, <<mem.static_boundaries>>@h
ori r8, r8, <<mem.static_boundaries>>@l
lis rOS, 0x8000
lbz r0, datahead.xBoundaries(rData)
lbz rCount, datahead.xCount(rData)
add rArr, rData, r0
load rAddr, MemGlob.addr
lwz r0, MemGlob.xSRAMLo(rAddr)
stw r0, 8<<2(rArr)  # HSD
lwz r0, MemGlob.xSRAMHi(rAddr)
stw r0, 9<<2(rArr)  # ArenaHi
lwz r0, OS.xFST(rOS)
stw r0, 10<<2(rArr) # FST
# update procedurally determined boundaries, beyond ArenaLo

cmplw rArg, rCount
lbz r0, datahead.xBoundaries(rData)
add rArr, rData, r0
mr r0, rArg
blt+ _ID

  _Addr:
  lis r0, 0x8180
  cmpw rArg, r0
  blt+ 0f

    _bad_input:
    li rID, -1
    li rAddr, 0
    li rString, 0
    blr  # if input was bad, then return nulls

  0:
  subic. rCount, rCount, 1
  slwi r0, rCount, 2
  blt- _bad_input
  lwzx r0, rArr, r0
  cmpw rArg, r0
  mr r0, rCount
  blt+ 0b

_ID:
mr rID, r0
addi rString, rData, datahead.size
slwi r0, rID, 2
lwzx rAddr, rArr, r0
lbzx r0, rString, rID
add rString, rData, r0
blr  # return ID, address, and string for valid inputs



<mem.static_boundaries> NTSC 1.02
count = 11
_head:  .byte _bounds-_head, _strings-_head, count
# 0x0 = offset of boundaries array
# 0x1 = offset of strings array
# 0x2 = number of boundary definitions
# 0x3 = (start of string offset byte array)

.altmacro; i = -1
.rept count;  i = i+1
  .irp n, %i; .byte \n\()f-_head; .endr
.endr; .noaltmacro; .align 2 # emit pointer bytes, to start of each null-terminated string
# - these are all offsets from base of this data table, at '_head'

_bounds:
.long 0x80000000  # 0 - "HEAD"    -- these are predictable static boundaries
.long 0x80005940  # 1 - "TEXT"
.long 0x803b7240  # 2 - "DATA"
.long 0x804316c0  # 3 - "BSS"
.long 0x804D36A0  # 4 - "SDATA"
.long 0x804D79e0  # 5 - "TOC"
.long 0x804DEC00  # 6 - "TEMP"
.long 0x804EEC00  # 7 - "LOW"
.long 0, 0, 0  # 8, 9, 10  -- these last ones are procedurally written, at runtime

_strings:
# strings are kept at end of table:
0:  .asciz "HEAD"
1:  .asciz "TEXT"
2:  .asciz "DATA"
3:  .asciz "BSS"
4:  .asciz "SDATA"
5:  .asciz "TOC"
6:  .asciz "RTS"
7:  .asciz "LOW"
8:  .asciz "HSD"
9:  .asciz "HIGH"
10: .asciz "FST"
.align 2



-==-

Data Functions

data.strcmp  # Compare 2 null-terminated strings
args: rA, rB
 <-- rBool

data.cmp  # compare 2 lengths of binary
args: rA, rB, rSize
 <-- rBool

data.point  # relocate various input types as pointer outputs
args: rAddr   # type 1: Address of branch instruction (or pointer)
args: rOffset, rAddr  # type 2: relocation from pointer
 <-- rPoint, rBase, rOffset
 <-- cr1.lt: bValidAddr,  cr1.gt: bIsBranch
# - all output pointers from branches are considered valid


data.zero  # fill out given area with zero
args: rData, rSize

data.fill  # fill out given area with a byte value
args: rData, rFill, rSize

data.copy  # Copies data from r4 over to r3
args: rDest, rSource, rSize
# - ARAM addresses are accepted for rSource/rDest
# - DMA request for ARAM is slow, but faster than DVD

data.copy_async  # Use async DMA queue with custom callback
args: rDest, rSource, rSize, rCallback, rCallbackArg
# - intended for ARAM <> SRAM copies
# - rCallback will be executed once copy has finished

  (sync callback arguments)
  args: rCallbackArg


data.async_info  # return info about pending async copies
args: rQuery  # for matching rDest, rSource, rCB, or rArg
 <-- rAsync, rDest, rSource, rSize, rCB, rArg, rGlob, rQuery
 <-- cr1.lt: bNotSynced,  cr1.gt: bMatch,  cr1.eq: bSynced

 rAsync  # Async ARAM Copy queue info struct...
0x00 : POINT : to next info struct in queue
0x04 : WORD  : Link ID -- 0=free, 1=async, 2=synced
0x08 : POINT : to other struct + 8 (no header) ???
0x0C : POINT : to header (offset 0x00)
0x10 : FLAG? : usually '1'
0x14 : ???
0x18 : POINT : to Source address, for copy  (SRAM or DRAM)
0x1C : POINT : to Destination address       (SRAM or DRAM)
0x20 : WORD  : Size of remaining copy job (in bytes)
0x24 : POINT : to Event Handler  (used by DMA queue?)
0x28 : POINT : to Sync Callback  (provided by user)
0x2C : WORD  : Callback Argument (for sync callback)

rGlob # ARAM Async Info Globals...
# - these correspond with the Link ID:
0x00 : POINT : to list of currently free descriptors
0x04 : POINT : to list of currently queued descriptors
0x08 : POINT : to list of synced descriptors ready for callback

data.sync_to  # sync to a matching .async_info query
args: rQuery
data.sync_until  # sync until a minimum of N queued asyncs
args: rCount
data.sync_next  # sync until the next queued async begins
data.sync  # sync all queued async data copies
# - all asyncs automatically sync when 4 or more asyncs are in queue
# - syncing will cause the CPU to stall until data copy completes

data.flush_DC  # flush data cache
args: rAddr, rSize
# - prevents race conditions with hardware accessing SRAM

data.flush_IC  # flush instruction cache
args: rAddr, rSize
# - forces written ppc instructions to be newly-interpreted

[Punkline]
<data.point> NTSC ALL
# relocate various input types as pointer outputs

# --- args (syntax 1):
# r3 = rAddr    -- address of branch instruction, pointer, or byte offset from rAddr
# --- args (syntax 2):
# r3 = rOffset  -- offset from base address
# r4 = rAddr    -- base address

# --- returns:
# r3 = rPoint   -- the resulting pointer address value
# r4 = rAddr    -- the base address used for this pointer
# r5 = rOffset  -- the offset used for this base address
# cr1.lt = bValidAddr  -- the resulting pointer is a valid address
# cr1.gt = bIsBranch   -- the given input was a branch instruction
.include "melee"; punkpc
lt=0; gt=1; eq=2; bIsValid=4; bIsBranch=5
lis r0, 0x8180
cmpw r3, r0
crclr bIsBranch
mr r5, r3
bge- 0f

  # if input r3 is an address
  lwz r5, 0(r3)  # load value
  cmpw r5, r0    # check if value is addr
  mr r4, r3      # (assume given address is base address)
  bge+ 1f

    # if loaded value is an address
    mr r4, r5
    li r5, 0
    b 0f  # then just use that address without an offset

  1: # if loaded value isn't an address
  rlwinm r3, r5, 0xC0000000
  cmpwi r3, 4  # check if it's a branch instruction opcode
  crmove bIsBranch, eq
  bne- 0f

    # if loaded value is a branch
    rlwinm r3, r5, 6, 0xFFFFFF00
    srawi r5, r3, 6  # then mask/extract signed word offset
    # else, just treat loaded value like a signed byte offset

0:
add r3, r4, r5
cmpw r3, r0
crmove bIsValid, lt
blr  # return updated cr1 bools, r3 pointer, and base/offset values



<data.zero> NTSC 1.02
b 0x8000c160
<data.fill> NTSC 1.02
b 0x80003130
<data.copy> NTSC 1.02
cmpwi r3, 0
bge- 0f
  cmpwi r4, 0
  bge- 0f
    b 0x800031f4
0: li r6, -1
b <data.copy_async>
# using -1 as callback arg causes auto-sync callback to be used



<data.copy_async> NTSC 1.02
# Copies data from r4 over to r3, allowing for ARAM addresses and sync callbacks
# - If a -1 is used in place of a callback address, this will become a sync copy instead of async
# - otherwise, the CPU will not wait for ARAM on return, allowing for immediate action
#   - the data being copied however will not be available for processing until ARAM has synced

# --- args:
# r3 = rCopy  - Copy data TO this address
# r4 = rFrom  - Copy data FROM this address
# r5 = rSize
# r6 = rSync  - sync callback, executes once the ARAM copy has finished
# r7 = rArg   - sync callback argument, gets passed as r3 to rSync when executed


max_count = 4
# this is the numebr of async queue slots (out of ~8) to allow for simultaneous use
# - if more than this many reads are queued, then func will wait to sync before starting new stream

_func_copy_async:
mflr r0
stwu sp, -0x40(sp)
stw  r0,  0x40+4(sp)
addi r10, sp, 0x10
stswi r3, r10, 0x1C
  li r3, max_count
  bl <data.sync_until>
  addi r10, sp, 0x10
lswi r3, r10, 0x1C

b _end_of_data
# - branch over inline data

  _get_data: blrl
    _sync_callback:
    li r0, 1
    stw r0, 0(r3)
    blr # set flag once synced

  _data:
  .long 0

_end_of_data:
cmpwi r6, -1
bne- _async
# -1 is a special keyword for auto-syncing (used by <copy.ARAM_sync>, and <copy>)

  _auto_sync:
  bl _get_data
  mflr r6
  li r0, 0
  addi r7, r6, _data - _sync_callback
  stw r0, 0(r7)
  stw r7, 0x10(sp)
  bl _func_ARAM_write
  # if -1 was detected, we set up an auto-sync callback

    _still_syncing:
    lwz r3, 0x10(sp)
    lwz r0, 0(r3)
    cmpwi r0, 0
    bne- _return
      b _still_syncing
      # loop until synced

_async:
bl _func_ARAM_write
# custom callbacks can handle sync on their own

_return:
lwz  r0,  0x40+4(sp)
addi sp, sp, 0x40
mtlr r0
blr

_func_ARAM_write:
mflr r0
lis r8, 0x8043
stw r0, 0x0004 (sp)
stwu sp, -0x0058 (sp)
stmw r23, 0x0034 (sp)
mr  r25, r4  # r25 = Source of copy
mr. r26, r3  # r26 = Destination of copy
mr  r27, r5  # r27 = Size
mr  r23, r6  # r23 = Callback
mr  r24, r7  # r24 = Callback Argument
ori r31, r8, 0x16c0
# r31 = base of ARAM DMA queue structure -- 804316c0

mr r4, r27   # r4 = size to invalidate or store in Data Cache (DC)
bge- 0f

  # --- ARAM to RAM
  li r29, 1
  mr r3, r26     # r3 = RAM destination,  r4 = size
  bl 0x803447DC  # DC Invalidate
  b 1f

  0: # --- RAM to ARAM
  li r29, 0
  mr r3, r25     # r3 = RAM source,  r4 = size
  bl 0x80344840  # DC Store

1: # r29 = copy op type, for queue request call arguments
# (we also swap the source/destination regs if writing, so that op type correctly selects them)

bl 0x80347364  # disable interrupts
addi r30, r3, 0
# r30 = backup interrupt flags

lwz r28, 0x1e0(r31)   # r28 = old queue pointer
stw r23, 0x28 (r28) # storing callback
stw r24, 0x2C (r28) # storing callback arg
mr r23, r29
# r23 and r24 are now safe to use

addi r29, r31, 0x1e0  # r29 = address of queue pointer
lwz r0, 0 (r28)       # r0 = link from old pointer
stw r0, 0 (r29)       # --- push stack with link from old pointer

addi r3, r31, 0x1e4
b 0f # seems to be some other list pointer at 0x4 of element

1:
mr r3, r0

0: lwz r0, 0 (r3)   # follow other list links until a null terminator is found
  cmplwi r0, 0
  bne+ 1b
  # r3 = last element in list

stw r28, 0 (r3)  # store old pointer in 0x4 of last element
li r3, 0
li r4, 1  # State of async
stw r3, 0x0(r28)
stw r4, 0x4(r28)  # some kind of metadata for informing the queue request

lis r3, 0x8001
addi r10, r3, 0x4AC4  # r10 = 80014AC4  -- callback event handler, for input callback and arg
addi r3, r28, 8  # r3 = base starting at 0x8 of element
mr r4, r28     # r4 = base of element (the metadata we just created)
mr r5, r23     # r5 = copy operation type (0 = write to ARAM,  1 = read from ARAM)
xori r6, r5, 1 # r6 = inverted type
mr r7, r25     # r7 = source
mr r8, r26     # r8 = destination
mr r9, r27     # r9 = size
b 0x80014c90  # branch into the middle of the ARAM read func from the Archive library
# - the stack frame and register context has been carefully designed to mimic this function

<data.strcmp> NTSC 1.02
b 0x803258e8
<data.cmp> NTSC 1.02
b 0x803258a8
<data.flush_DC> NTSC 1.02
b 0x8034480c
<data.flush_IC> NTSC 1.02
b 0x8000543c



<data.async_info> NTSC 1.02
# Get info about queued ARAM DMA copy jobs based on input query

# --- args:
# r3 = rQuery -- compared against rAsync, rDest, rSource, rCB, rArg for matches

# --- returns:
# cr1.lt = bNotSynced  -- 'matched async is not done copying'
# cr1.gt = bMatch      -- 'match found'
# cr1.eq = bSynced     -- 'no match found' (if input was valid, this means it synced already)
# r3 = rAsync   -- async info (null if no match)
# r4 = rDest   -- copy destination
# r5 = rSource -- copy source
# r6 = rSize   -- copy size
# r7 = rCB     -- callback argument
# r8 = rArg    -- callback address
# r9 = rGlob   -- global pointers
# r10 = rQuery -- (given query)

# --- ARAM Async Info descriptor - 10 (?) 0x30-byte structures starting at 0x804316c0
# 0x00 : POINT : to next info struct in queue
# 0x04 : WORD  : Link ID -- 0=free, 1=async, 2=synced
# 0x08 : POINT : to other struct + 8 (no header) ???
# 0x0C : POINT : to header (offset 0x00)
# 0x10 : FLAG? : usually '1'
# 0x14 : ???
# 0x18 : POINT : to Source address, for copy  (SRAM or DRAM)
# 0x1C : POINT : to Destination address       (SRAM or DRAM)
# 0x20 : WORD  : Size of remaining copy job (in bytes)
# 0x24 : POINT : to Event Handler  (used by DMA queue?)
# 0x28 : POINT : to Sync Callback  (provided by user)
# 0x2C : WORD  : Callback Argument (for sync callback)

# --- ARAM Async Info Globals - starting at 0x804316c0 + 0x1E0
# - these correspond with the Link ID:
# 0x00 : POINT : to list of currently free descriptors
# 0x04 : POINT : to list of currently queued descriptors
# 0x08 : POINT : to list of synced descriptors ready for callback


.include "melee"
punkpc ppc
melee mem
regs (r3), +1, rAsync, rDest, rSource, rSize, rCB, rArg, rGlob, rQuery
enum (cr1.lt), bNotSynced, bMatch, bSynced   # returned bools
bLast = bNotSynced
mr rQuery, r3
cmpwi rQuery, -1
load rGlob, data.async_glob.addr
lwz r0, data.async_glob.xAsync(rGlob)
crclr bMatch
crmove bLast, eq
# if the query is given as -1, then return last available async in queue

_for_each_pending:
  mr. rAsync, r0
  bge- _return
  # return without updating default cr1 values if no match is found in queued

  lwz r0, data.async_info.xNext(rAsync)
  cmpwi r0, 0
  crandc bMatch, bLast, lt
  cmpw rAsync, rQuery
  cror bMatch, bMatch, eq
  lwz rDest, data.async_info.xDest(rAsync)
  cmpw rDest, rQuery
  lwz rSource, data.async_info.xSource(rAsync)
  cror bMatch, bMatch, eq
  cmpw rSource, rQuery
  lwz rCB, data.async_info.xSyncCB(rAsync)
  cror bMatch, bMatch, eq
  cmpw rCB, rQuery
  lwz rArg, data.async_info.xSyncArg(rAsync)
  cror bMatch, bMatch, eq
  cmpw rArg, rQuery
  lwz rSize, data.async_info.xSize(rAsync)
  cror bMatch, bMatch, eq
  bf+ bMatch, _for_each_pending

_return:
crnot bSynced, bMatch
crnot bNotSynced, bSynced
# set bools according to bMatch
# - these are formatted to be similar to DVD async_info returns, even though they are redundant...

blr



<data.sync> NTSC 1.02
li r3, -1
b <data.sync_to>

<data.sync_next> NTSC 1.02
lis r3, 0x8043
lwz r3, 0x16c0+0x1e4(r3)
b <data.sync_to>

<data.sync_to> NTSC 1.02
.include "melee"
punkpc ppc
melee mem
prolog rMSR

mfmsr rMSR
rlwinm r0, rMSR, 0, ~0x8000
mtmsr r0
bl <data.async_info>
# get async info without interrupts enabled, to prevent messing up return epilog

bt- data.async_info.bSynced, _return
# skip if query wasn't found, or currenty in the middle of sync event...

  ori r0, rMSR, 0x8000
  mtmsr r0
  # else enable interrupts while syncing in continuous thread...

  _while_not_synced:
    lwz r0, data.async_glob.xSynced(data.async_info.rGlob)
    cmpwi r0, 0
    blt- _return
    # abort if a sync event is ready...

    lwz r0, data.async_info.xStatus(data.async_info.rAsync)
    cmpwi r0, 1
    beq+ _while_not_synced
    # wait until status flag changes on sync...

_return:
mtmsr rMSR
epilog
blr



<data.sync_until> NTSC 1.02
.include "melee"
punkpc ppc
melee mem

regs (r3), +1, rTarget, rBase, rCount, rX,
load rBase, data.async_info.addr

_try_again:
lwz r0, (data.async_glob.addr + data.async_glob.xSynced) - data.async_info.addr (rBase)
cmpwi r0, 0
bltlr-
# skip if currently in sync event...

li rCount, 0
li rX, (8*data.async_info.size) + data.async_info.xStatus
  _while_target_not_reached:
    subic. rX, rX, data.async_info.size
    blt- 0f
      lwzx r0, rBase, rX
      cmpwi r0, 1
      bne- _while_target_not_reached
        addi rCount, rCount, 1
        b _while_target_not_reached
        # count the number of pending async copies
  0:
  cmpw rTarget, rCount
  blt- _try_again
  # if the count > target, then wait until some have synced by trying again

blr
